{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaw_estimation.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "importlib.reload(models)\n",
    "cuda = torch.device('cuda')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_002.torch\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"fast_tunnel_traversal\"\n",
    "DATASET_FOLDER = f\"/home/lorenzo/datasets/{DATASET}\"\n",
    "MODEL = models.YawEstimator\n",
    "SAVE_FOLDER = f\"/home/lorenzo/models/gallery_detection/procedural_datasets/{DATASET}\"\n",
    "n_epochs = int(64*2)\n",
    "batch_size = 128\n",
    "lr = 0.002\n",
    "try:\n",
    "    os.mkdir(SAVE_FOLDER)\n",
    "except:\n",
    "    pass\n",
    "save_path = os.path.join(SAVE_FOLDER,MODEL.__name__+f\"-_bs{batch_size}_ne{n_epochs}_lr{str(lr).replace('.','_')}.torch\")\n",
    "print(save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_001\n",
      "env_002\n",
      "env_003\n",
      "env_004\n",
      "env_005\n",
      "env_006\n",
      "env_007\n",
      "env_008\n",
      "env_009\n",
      "env_010\n",
      "env_011\n",
      "env_012\n",
      "env_013\n",
      "env_014\n",
      "env_015\n",
      "env_016\n",
      "env_017\n",
      "env_018\n",
      "env_019\n",
      "env_020\n",
      "9999\r"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, path_to_dataset, max_elements_per_env=None):\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\")\n",
    "        self.load_dataset(path_to_dataset,max_elements_per_env=max_elements_per_env)\n",
    "        self.data_augmentation=torch.nn.Sequential(\n",
    "            transforms.RandomErasing()\n",
    "        )\n",
    "    def load_dataset(self, dataset_folder, max_elements_per_env = None):\n",
    "        n_datapoints = 0\n",
    "        envs = os.listdir(dataset_folder)\n",
    "        envs.sort()\n",
    "        for env in envs:\n",
    "            env_folder = os.path.join(dataset_folder, env)\n",
    "            data_folder = os.path.join(env_folder, \"data\")\n",
    "            if os.path.isdir(data_folder):\n",
    "                if max_elements_per_env is None:\n",
    "                    n_datapoints += len(os.listdir(data_folder))\n",
    "                else:\n",
    "                    n_datapoints+=min(len(os.listdir(data_folder)),max_elements_per_env)\n",
    "            else:\n",
    "                envs.remove(env)\n",
    "        self.n_datapoints = n_datapoints\n",
    "        self.labels = torch.zeros((self.n_datapoints, 1)).float()\n",
    "        self.images = torch.zeros((self.n_datapoints,1,30,30)).float()\n",
    "        n = 0\n",
    "        for env in envs:\n",
    "            print(env)\n",
    "            env_folder = os.path.join(dataset_folder, env)\n",
    "            data_folder = os.path.join(env_folder, \"data\")\n",
    "            dtp_names = os.listdir(data_folder)\n",
    "            dtp_names.sort()\n",
    "            if not max_elements_per_env is None:\n",
    "                dtp_names = dtp_names[0:min(len(dtp_names),max_elements_per_env)]\n",
    "            for dtp_n, dtp_name in enumerate(dtp_names):\n",
    "                print(f\"{dtp_n:04d}\",end=\"\\r\")\n",
    "                dtp_path = os.path.join(data_folder, dtp_name)\n",
    "                dtp = np.load(dtp_path)\n",
    "                self.labels[n, :] = torch.tensor(dtp[\"label\"])\n",
    "                self.images[n,0, :,:] = torch.tensor(dtp[\"image\"])\n",
    "                n+=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_datapoints\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx, ...]\n",
    "        result = self.labels[idx, ...]\n",
    "        return image.float(), result.float()\n",
    "    def delete(self, idx):\n",
    "        self.images = torch.cat(self.images[0:idx,...],self.images[idx+1,:])\n",
    "        self.labels= torch.cat(self.labels[0:idx,...],self.labels[idx+1,:])\n",
    "\n",
    "dataset = ImageDataset(DATASET_FOLDER,max_elements_per_env=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28.6240])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, result = dataset[n]\n",
    "n+=1\n",
    "print(np.rad2deg(result))\n",
    "plt.imshow(image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(dataset,[0.9,0.1])\n",
    "torch.cuda.empty_cache()\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=5)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=5)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_train(network, train_loader, criterion, optimizer, n_epochs,lr,tensorborad_folder=\"/home/lorenzo/tensor_board\"):\n",
    "    shutil.rmtree(tensorborad_folder) \n",
    "    os.mkdir(tensorborad_folder)\n",
    "    writer = SummaryWriter(log_dir=tensorborad_folder)\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        print(\"\", end=\"\\r\")\n",
    "        print(\"Epoch {} out of {}\".format(\n",
    "            epoch + 1, n_epochs), end=\"\")\n",
    "        for i, data in enumerate(train_loader):\n",
    "            torch.cuda.empty_cache()\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(torch.device(\"cuda\"))\n",
    "            labels = labels.to(torch.device(\"cuda\"))\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            j = i + epoch * train_loader.__len__()\n",
    "            writer.add_scalar(f\"Loss/train/lr_{lr}\",loss,j)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open(\"/home/lorenzo/models/gallery_detection/procedural_datasets/dataset_03/gallery_detector_v3-_r10_lr002_3.torch\", \"rb\") as f:\n",
    "        network.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100000\n",
      "0.065000\n",
      "0.042250\n",
      "0.027463\n",
      "0.017851\n",
      "0.011603\n",
      "0.007542\n",
      "0.004902\n",
      "0.003186\n",
      "0.002071\n",
      "0.001346\n",
      "0.000875\n",
      "0.000569\n",
      "0.000370\n",
      "0.000240\n",
      "0.000156\n",
      "0.000102\n",
      "0.000066\n",
      "0.000043\n",
      "0.000028\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.1*0.65**n for n in range(20)]\n",
    "for i in lrs:\n",
    "    print(f\"{i:05f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_100000.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_065000.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_042250.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_027463.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_017851.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_011603.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_007542.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_004902.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_003186.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_002071.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_001346.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000875.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000569.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000370.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000240.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000156.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000102.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000066.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000043.torch\n",
      "Epoch 128 out of 128/home/lorenzo/models/gallery_detection/procedural_datasets/fast_tunnel_traversal/YawEstimator-_bs128_ne128_lr0_000028.torch\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "for lr in lrs:\n",
    "    network = MODEL().to(cuda).float()\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        network.parameters(),\n",
    "        lr=lr,\n",
    "    )\n",
    "    loss_hist = basic_train(\n",
    "        network, train_dataloader, criterion, optimizer, n_epochs, lr\n",
    "    )\n",
    "    lr_str = f\"{lr:04f}\".replace(\".\", \"_\")\n",
    "    save_path = os.path.join(SAVE_FOLDER,MODEL.__name__+f\"-_bs{batch_size}_ne{n_epochs}_lr{lr_str}.torch\")\n",
    "    print(save_path)\n",
    "    network.to(\"cpu\")\n",
    "    torch.save(network.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/models/gallery_detection/procedural_datasets/dataset_03/gallery_detector_v3-_r10_lr002_5.torch\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/lorenzo/models/gallery_detection/procedural_datasets/dataset_03/gallery_detector_v3-_r10_lr002_5.torch\"\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
